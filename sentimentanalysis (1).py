# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12VR3LokqbN93joDrhrbBYw3g4jbDTCfJ
"""

!pip install kaggle

!mkdir  -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#api to fecth dataset from kaggle
!kaggle datasets download -d kazanova/sentiment140

#unzip th zip file
from zipfile import ZipFile
dataset= "/content/sentiment140.zip"

with ZipFile(dataset,'r') as zip:
    zip.extractall()
    print('The Dataset is extracted')

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

#printing stopwords into English
print(stopwords.words('english'))

#dataprocessing
#loading data from csv to pandas dataframe
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding ='ISO-8859-1')

#checking the no.of rows and coln
twitter_data.shape

#first 5 vrows
twitter_data.head()

#naming the colns and reading dt agian
column_names=['target','id','data','flag','user','text']
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',names=column_names, encoding ='ISO-8859-1')

#first 5 vrows
twitter_data.head()

#counting the no.of missing values
twitter_data.isnull().sum()

#checlking the distribution of target colns
twitter_data['target'].value_counts()

#converting the labele int 0 and 1
twitter_data.replace({'target':{4:1}},inplace=True)

twitter_data['target'].value_counts()

#zero=negative tweet 1=positive tweet
#Stemming
port_stem = PorterStemmer()

def stemming(content):

  stemmed_content = re.sub('[^a-zA-Z]',' ',content)#remove the words which aren't alphabets
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content =' '.join(stemmed_content)

  return stemmed_content

twitter_data['stemmed_content']=twitter_data['text'].apply(stemming)

twitter_data.head()

print(twitter_data['stemmed_content'])

print(twitter_data['target'])

#spearating the data and label
X=twitter_data['stemmed_content'].values
Y=twitter_data['target'].values

print(X)

print(Y)

"""splitting the data into training data and test data"""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)

print(X.shape,X_train.shape,X_test.shape)

#feature Extraction,converting data into numerical data

vectorizer = TfidfVectorizer()

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

print(X_train)

print(X_test)

"""Training ML model

Logistic Regression
"""

model = LogisticRegression(max_iter=1000)

model.fit(X_train,Y_train)

"""Model Evalution"""

#Accuracy Score on training data
X_train_prediction=model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train,X_train_prediction)

print('accuracy Score on training data:',training_data_accuracy)

#Accuracy Score on test data
X_test_prediction=model.predict(X_test)
testing_data_accuracy = accuracy_score(Y_test,X_test_prediction)

print('accuracy Score on testing data:',testing_data_accuracy)

"""Model Accuracy=77.8%

Saving the trained model
"""

import pickle

filename='trained_model.sav'
pickle.dump(model,open(filename,'wb'))

"""Using saved modfel for future prediction"""

#loading the saved model
loaded_model=pickle.load(open('trained_model.sav','rb'))

X_new = X_test[200]
print(Y_test[200])

prediction= model.predict(X_new)
print(prediction)

if(prediction[0]==0):
  print('Negative Tweet')
else:
  print('Positive Tweet')

X_new = X_test[3]
print(Y_test[3])

prediction= model.predict(X_new)
print(prediction)

if(prediction[0]==0):
  print('Negative Tweet')
else:
  print('Positive Tweet')

X_new = X_test[20]
print(Y_test[20])

prediction= model.predict(X_new)
print(prediction)

if(prediction[0]==0):
  print('Negative Tweet')
else:
  print('Positive Tweet')

X_new = X_test[221]
print(Y_test[221])

prediction= model.predict(X_new)
print(prediction)

if(prediction[0]==0):
  print('Negative Tweet')
else:
  print('Positive Tweet')

X_new = X_test[111]
print(Y_test[111])

prediction= model.predict(X_new)
print(prediction)

if(prediction[0]==0):
  print('Negative Tweet')
else:
  print('Positive Tweet')

!pip install ipywidgets

import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from ipywidgets import widgets, interact
from IPython.display import display, HTML

# Load the trained model and vectorizer (make sure to save your vectorizer along with the model)
model = pickle.load(open('trained_model.sav', 'rb'))

# Function for prediction
def predict_sentiment(tweet):
    processed_text = vectorizer.transform([tweet])  # Ensure your vectorizer matches model training
    prediction = model.predict(processed_text)
    sentiment = "Positive" if prediction[0] == 1 else "Negative"
    color = "green" if sentiment == "Positive" else "red"
    display(HTML(f"<h3>Predicted Sentiment: <span style='color: {color};'>{sentiment}</span></h3>"))

# Create a text input widget
tweet_input = widgets.Text(
    description='Tweet:',
    placeholder='Enter a tweet...',
    layout=widgets.Layout(width='70%')
)

# Create a button widget
button = widgets.Button(description="Analyze Sentiment")

# Define button action
def on_button_click(b):
    predict_sentiment(tweet_input.value)

button.on_click(on_button_click)

# Display widgets
display(tweet_input, button)